\begin{table*}[t]
\processtable{ROC-AUC scores of different GRN inference methods on 3 yeast expression datasets and a LCL dataset from MERLIN-P, evaluated on 3 and 2 goldstandard networks, respectively.\label{tab:merlin-p-benchmark}}{\begin{tabular}{lccccccccccr} \\ \toprule
Method & \multicolumn{2}{c}{LCL (Niu)} & \multicolumn{2}{c}{LCL (Geuvadis)} & \multicolumn{2}{c}{NatVar (Average)} & \multicolumn{2}{c}{KO (Average)} & \multicolumn{2}{c}{Stress (Average)} & Overall score \\
 \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11}  
 & AUPR & AUROC & AUPR & AUROC & AUPR & AUROC & AUPR & AUROC & AUPR & AUROC &  \\ \midrule
ARACNe-AP & 0.137 & 0.503 & 0.134 & 0.493 & 0.034 & 0.578 & 0.019 & 0.521 & 0.022 & 0.548 & 3.687 \\
GENIE3 & 0.125 & 0.482 & 0.137 & 0.501 & 0.015 & 0.481 & 0.016 & 0.506 & 0.016 & 0.502 & 0.323 \\
PLSNET & 0.130 & 0.484 & 0.118 & 0.468 & 0.033 & 0.523 & 0.015 & 0.488 & 0.019 & 0.514 & 14.977 \\
TIGRESS & 0.138 & 0.500 & \textbf{0.150} & \textbf{0.520} & 0.020 & 0.498 & 0.020 & 0.520 & 0.015 & 0.497 & 1.587 \\
ENNET & 0.128 & 0.491 & 0.128 & 0.483 & 0.037 & 0.569 & 0.024 & 0.521 & 0.028 & 0.536 & 17.463 \\
\midrule
\fastmethodname & 0.140 & 0.502 & 0.141 & 0.502 & 0.110 & 0.657 & \textbf{0.029} & 0.552 & 0.031 & \textbf{0.559} & 45.852 \\
\methodname & \textbf{0.140} & \textbf{0.509} & 0.140 & 0.505 & \textbf{0.111} & \textbf{0.660} & 0.028 & \textbf{0.552} & \textbf{0.031} & 0.559 & \textbf{45.891} \\
\bottomrule
\end{tabular}}{}
\end{table*}
